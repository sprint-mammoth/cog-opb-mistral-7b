# Configuration for Cog ⚙️
# Reference: https://github.com/replicate/cog/blob/main/docs/yaml.md

build:
  # set to true if your model requires a GPU
  cuda: "11.7"
  gpu: true

  # a list of ubuntu apt packages to install
  # system_packages:
    # - "libgl1-mesa-glx"
    # - "libglib2.0-0"

  # python version in the form '3.8' or '3.8.12'
  python_version: "3.10"

  # a list of packages in the format <package-name>==<version>
  # Requires: Transformers 4.33.0 or later, Optimum 1.12.0 or later, and AutoGPTQ 0.4.2 or later.
  python_packages:
    - torch==2.0.1
    # - torchvision==0.15.2
    - transformers
    - accelerate
    - optimum
    - safetensors
    - huggingface-hub 
    - hf-transfer
  
  # commands run after the environment is setup
  run:
    - pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu117/
    # - CUDA_HOME=/usr/local/cuda pip install git+https://github.com/vllm-project/vllm.git@main

# predict.py defines how predictions are run on your model
predict: "predict.py:Predictor"