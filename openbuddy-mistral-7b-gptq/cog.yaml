# Configuration for Cog ⚙️
# Reference: https://github.com/replicate/cog/blob/main/docs/yaml.md

build:
  # set to true if your model requires a GPU
  gpu: true
  cuda: "11.8"

  # a list of ubuntu apt packages to install
  # system_packages:
    # - "libgl1-mesa-glx"
    # - "libglib2.0-0"

  # python version in the form '3.8' or '3.8.12'
  python_version: "3.10"

  # a list of packages in the format <package-name>==<version>
  # Requires: Transformers 4.33.0 or later, Optimum 1.12.0 or later, and AutoGPTQ 0.4.2 or later.
  python_packages:
    - torch
    - transformers
    - accelerate 
    - optimum 
    - safetensors 
    - huggingface-hub 
    - hf-transfer
  
  # commands run after the environment is setup
  run:
    - pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/
    - mkdir openbuddy
    - ls -l
    - export HF_HUB_ENABLE_HF_TRANSFER=1
    - echo $HF_HUB_ENABLE_HF_TRANSFER
    - huggingface-cli download TheBloke/openbuddy-mistral-7B-v13-GPTQ --local-dir openbuddy --local-dir-use-symlinks False
    # - CUDA_HOME=/usr/local/cuda pip install git+https://github.com/vllm-project/vllm.git@main

# predict.py defines how predictions are run on your model
predict: "predict.py:Predictor"